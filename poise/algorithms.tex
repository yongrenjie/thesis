\subsection{Optimisation algorithms}
\label{subsec:poise__algorithms}

As was briefly mentioned in \cref{subsec:pureshift__optim_techniques}, derivative-based optimisation algorithms cannot be used for experimental optimisations.
To be more precise, when analytical gradients are not available, derivative-based algorithms calculate gradients using a finite difference approximation:
\begin{equation}
    \label{eq:finite_difference_grad}
    \nabla f(x) \approx \frac{f(x + \varepsilon) - f(x - \varepsilon)}{2\varepsilon},
\end{equation}
where $\varepsilon$ is the step size used for the finite difference calculation.
In Nocedal and Wright\autocite{Nocedal2006}, it is shown that the error in this finite difference gradient (as compared to the true gradient) has an upper bound of
\begin{equation}
    \label{eq:finite_difference_error}
    \frac{\eta(x;\varepsilon)}{\varepsilon} + O(\varepsilon^2),
\end{equation}
where $\eta(x;\varepsilon)$ is the noise in the region $[x - \varepsilon, x + \varepsilon]$.
If $\varepsilon$ is small, the first term (the error due to noise) is large, and if $\varepsilon$ is large, the second term (which is the error due to the finite difference approximation) is large.
This means that finite difference gradients, and any algorithms which use them, are entirely unreliable in the presence of (sufficient) noise.
Instead, POISE provides a choice of three derivative-free optimisation algorithms: the \acf{nm} method\autocite{Nelder1965TCJ}, the \acf{mds} method\autocite{Torczon1989,DennisJr1991SIAMJO}, and the Py-BOBYQA trust-region method\autocite{Powell2009Proc,Cartis2019ACMTMS}.


\subsubsection{Nelder--Mead}

The NM method is a highly popular derivative-free optimisation algorithm, which maintains a set of points $\{y_1, y_2, \cdots, y_{n+1}\}$ during the optimisation, where $n$ is the number of parameters being optimised.
The convex hull of these points, $Y$, is the smallest possible set of points containing all the $y_k$ such that
\begin{equation}
    \label{eq:convex_hull}
    \forall x_1, x_2 \in Y, \forall \alpha \in [0, 1], \alpha x_1 + (1 - \alpha) x_2 \in Y,
\end{equation}
and is called a \textit{simplex}.
To provide an analogy for $n = 2$, the convex hull is the shape obtained by stretching a rubber band around three pins placed at $y_1, y_2, y_3$.
If this convex hull is nonempty---or equivalently, if the $n$ vectors $y_k - y_1$ ($2 \leq k \leq n + 1$) are linearly independent---then the simplex is called \textit{nonsingular}.
(In the $n = 2$ case, the convex hull would be empty if the three points were collinear.)

\begin{figure}[htb]
    \centering
    \includegraphics[]{poise/neldermead.png}
    \caption[Trial points in an iteration of the Nelder--Mead algorithm]{
        Diagram showing various points evaluated in one iteration of the Nelder--Mead algorithm (for an optimisation of two parameters).
        The solid black lines indicate the current simplex, which is assumed to be ordered such that $y_1$ is the best point (has the lowest cost function value) and $y_3$ the worst.
        The blue dots indicate the trial points which the algorithm attempts to replace $y_3$ with, and are further discussed in the text.
        Blue dashed lines indicate the simplex which would result if the corresponding trial point is accepted.
    }
    \label{fig:neldermead}
\end{figure}

The NM algorithm is in fact quite intuitive to understand.
The initial simplex is first constructed using the supplied initial point: POISE specifically uses the method of Spendley et al.\autocite{Spendley1962T}
The optimisation itself begins by measuring the cost function $f$ at every point of the simplex, and sorting the points in ascending order of cost function values (i.e.\ from best to worst), such that $f(y_1) \leq f(y_2) \leq \cdots \leq f(y_{n+1})$.
The centroid of the simplex is defined by the best $n$ points,
\begin{equation}
    \label{eq:simplex_centroid}
    \bar{y} = \sum_{i=1}^n y_i.
\end{equation}
On each iteration of the NM algorithm, we attempt to replace the worst point $y_{n+1}$  with a better point (\cref{fig:neldermead}).
The search for the new point is performed in several steps: first, the worst point is \textit{reflected} about the centroid of the simplex to obtain a new point:
\begin{equation}
    \label{eq:nm_reflect}
    y_\text{r} = \bar{y} - (y_{n+1} - \bar{y}).
\end{equation}
The value of the cost function is evaluated at this point, and is critical in determining how the algorithm proceeds.
If this reflected point falls in the middle of the pack, such that $f(y_1) \leq f(y_\text{r}) < f(y_n)$, this represents a `modest' improvement in the cost function: we simply replace the worst point with this and continue to the next iteration.

On the other hand, if the reflected point is better than all the other points (i.e.\ $f(y_\text{r}) < f(y_1)$), then we ambitiously attempt to \textit{expand} the simplex even further in that direction:
\begin{equation}
    \label{eq:nm_expand}
    y_\text{e} = \bar{y} - 2(y_{n+1} - \bar{y}).
\end{equation}
Of course, there is no guarantee that this is necessarily better than $y_\text{r}$; therefore, we choose whichever point of $y_\text{r}$ or $y_\text{e}$ had a lower value of $f$, and replace the worst point with this and continue to the next iteration.

If the reflected point is an improvement on the worst point but is no better than the remaining points, in that $f(y_n) \leq f(y_\text{r}) < f(y_{n+1})$, then the algorithm performs an \textit{outside contraction}, which resembles a half-hearted reflection:
\begin{equation}
    \label{eq:nm_outside_contract}
    y_\text{oc} = \bar{y} - (1/2)(y_{n+1} - \bar{y}).
\end{equation}
Conversely, if the reflected point is even worse than the worst point ($f(y_{n+1}) \leq f(y_\text{r})$), then this suggests that that search direction is very poor: we thus perform an \textit{inside contraction}, which uses a point halfway between the worst point and the centroid:
\begin{equation}
    \label{eq:nm_inside_contract}
    y_\text{oc} = \bar{y} + (1/2)(y_{n+1} - \bar{y}).
\end{equation}
If either of these contracted points are any better than $y_\text{r}$, then we replace the worst point in the simplex and continue to the next iteration; otherwise, we conclude that no search direction was good, and simply shrink the simplex towards the current best point by replacing each point $y_k$ with $(y_k + y_1)/2$.
In practice, these `last-resort' shrink steps occur very rarely.

Finally, convergence is signalled when for each dimension of the optimisation, the width of the simplex is smaller than the chosen optimisation tolerance.%
\footnote{The implementation of the NM algorithm in the \texttt{scipy} library only accepts a single value for the `tolerance', which is then used in all dimensions.
This is designed to be used by scaling the parameters beforehand such that the tolerance in each dimension is equal (and in fact, POISE was later updated to do so).
However, during initial development I chose instead to re-implement the NM algorithm with a convergence check which allowed for different tolerances to be specified in each dimension.}
For a multiple-parameter optimisation, this can potentially mean that extra accuracy is obtained in one of the parameters (because the simplex may have shrunk along that dimension more quickly).
However, it does guarantee that \textit{at least} the specified level of accuracy in every dimension is achieved.


\subsubsection{Multidirectional search}

In the preceding discussion, we noted that the simplex $Y$ was nonsingular if the $n$ vectors $y_k - y_1$ were linearly independent.
Equivalently, the matrix $M$ formed by concatenating these vectors
\begin{equation}
    \label{eq:simplex_matrix}
    M = \begin{pmatrix}
        y_2 - y_1, y_3 - y_1, \cdots, y_{n+1} - y_1 \\
    \end{pmatrix}
\end{equation}
must be nonsingular, i.e.\ have a nonzero determinant.
We can quantify how `close' the simplex $Y$ is to being singular, using the $l^2$ condition number of the matrix $M$, which in this context is usually referred to as the \textit{simplex condition}:
\begin{equation}
    \label{eq:simplex_condition}
    \kappa(Y) = \lVert M \rVert \lVert M^{-1} \rVert,
\end{equation}
where $\lVert M \rVert$ is the matrix norm induced by the Euclidean norm,
\begin{equation}
    \label{eq:matrix_norm}
    \lVert M \rVert = \max_{x \neq 0} \frac{\lVert Mx \rVert}{\lVert x \rVert}.
\end{equation}
A singular simplex $Y$ of course does not have a well-defined condition, since $M^{-1}$ does not exist.
However, the larger the condition of a simplex is, the closer it is to being singular.
Very loosely speaking, a long and thin simplex has a large condition number, and would be singular if its width were to go to zero.

The simplex updates made in the process of the NM algorithm mean that the simplex condition changes throughout the course of the optimisation.
This is good for achieving decreases in the cost function, since the simplex shape \textit{adapts} to the cost function being optimised.
However, if the simplex condition gets too large, it is possible that the optimisation will stall at a nonstationary point, since the search directions of the simplex are severely limited.
The MDS algorithm was proposed partially for the purpose of avoiding this ill-conditioning.\footnote{The main reason was in fact to better exploit computer parallelism, but it was also noticed that the MDS method proved to be generally more robust than NM.}
The MDS method is also simplex-based, and uses similar reflection/expansion/contraction steps as NM.
However, instead of (e.g.) reflecting a single worst point $y_{n+1}$ about the other points, it reflects all of the $n$ worst points $\{y_2, y_3, \cdots, y_{n+1}\}$ about the best point $y_1$.
This means that the shape of the simplex, and thus its condition number, is always preserved, which provides it with much better convergence properties\autocite{Torczon1989,Torczon1991SIAMJO}.%
\footnote{Specifically, it can be concluded that at least one of the search directions was bounded away from being orthogonal to the gradient; or in simpler (and less precise) terms, at least one of the search directions is close enough to a direction in which the cost function $f$ decreases.}

The increased reliability of the MDS algorithm over the NM algorithm was demonstrated on a variety of example optimisation problems: even in the very simple case where the cost function was simply the norm of a vector,
\begin{equation}
    \label{eq:norm_cf}
    f(y) = \lVert y \rVert,
\end{equation}
it was shown that the NM algorithm stalled when the dimension of the problem, $n$, was sufficiently large.
The value of $n$ needed to precipitate this failure depended on the problem being solved, and generally ranged from 8 to 40.
On the other hand, the MDS method proved to be robust under the same conditions, eventually converging to the optimum---although in the cases where NM \textit{did} work, the MDS method generally required more FEs.

It was this improved robustness of the MDS algorithm which prompted Goodwin et al.\autocite{Goodwin2018JMR} to use it in their (experimental) optimisation of ESR pulse shapes, and for me to later include it in POISE.
In the ESR work, the number of pulse points being optimised was 11 or 21, which fell into the regime where the MDS method would likely have better convergence properties than NM.
However, optimisations of this scale are feasible in ESR only because of the rapid relaxation and thus short experiment repetition times.
In NMR, each experiment takes a substantially longer time, and even optimisations with $n > 2$ become rather time-consuming due to the number of FEs required (the largest $n$ explored in the present work is 4).
As will be shown later, we found that the NM and MDS methods were equally reliable in our optimisations, with NM generally being faster.


\subsubsection{Py-BOBYQA}

Unlike the NM and MDS algorithm, Py-BOBYQA is not simplex-based, but is a trust-region algorithm.\autocite{Powell2009Proc,Cartis2019ACMTMS}
The fundamental idea behind a (derivative-free) trust-region method is to sample the cost function at a set of points $Y$, and construct a model $m$ through interpolation, which matches the cost function at these points:
\begin{equation}
    \label{eq:trust_region_model}
    \forall y \in Y, m(y) = f(y).
\end{equation}
The model at iteration $k$ is labelled $m_k$.
Most trust region methods use a quadratic model, and Py-BOBYQA is no exception.
This can be expressed as:
\begin{equation}
    \label{eq:trust_region_quadratic_model}
    m_k(x_k + p) = c + g^Tp + p^TGp,
\end{equation}
where $G$ is a symmetric matrix and $x_k$ is the centre of the model at iteration $k$ ($x_0$ being the user-specified initial point).
For this model to be fully determined, the set $Y$ must therefore contain $(n+1)(n+2)/2$ points in total.%
\footnote{In a derivative-based trust region method, $g$ and $G$ are determined using information from the gradient and/or Hessian.}

The algorithm maintains a \textit{trust region radius} $\Delta_k$ at each iteration, which is a measure of how reliable the model is.
The initial trust region radius, $\Delta_0$, can be arbitrarily chosen: in the case of POISE, I elected to set $\Delta_0$ to be $10$ times the desired tolerance.
The model $m_k$ is then used to calculate the next step $s_k$, which is obtained by minimising $m_k$ over all points within a radius of $\Delta_k$ from the centre $x_k$ (the \textit{trust region subproblem}):
\begin{equation}
    \label{eq:trust_region_subproblem}
    s_k = \argmin_{\lVert s \rVert \leq \Delta_k} m_k(x_k + s).
\end{equation}
Since $m_k$ is noiseless, this can be done with almost any algorithm: Py-BOBYQA uses a conjugate gradient method.
The (true) cost function is then evaluated at the trial point $x_k + s_k$, and compared against the value predicted by the model.
If the ratio of `actual improvement' to `predicted improvement' is large enough, i.e.
\begin{equation}
    \label{eq:trust_region_threshold}
    r_k = \frac{f(x_k) - f(x_k + s_k)}{m_k(x_k) - m_k(x_k + s_k)} \geq \eta
\end{equation}
for some threshold value $\eta$, then the step $s_k$ is accepted and $x_{k+1}$ is set to $x_k + s_k$, replacing the worst point in $Y$.
Additionally, the trust region radius $\Delta_k$ may be increased so that the next step(s) can be more ambitious.
Conversely, if $r_k < \eta$, then there are one of two possibilities: either the model is poorly conditioned (in that the points in $Y$ are very unevenly distributed), in which case one of the points is replaced and the model recalculataed; or the model is sufficiently well-conditioned, in which case the step is rejected, and $\Delta_k$ is decreased.

Py-BOBYQA goes beyond a standard derivative-free trust-region algorithm in further limiting the rate at which the radius $\Delta_k$ can change (amongst others).
Separately from $\Delta_k$, Py-BOBYQA also maintains a lower bound on the trust region radius $\rho_k$, and on unsuccessful iterations $\Delta_k$ is not allowed to decrease further than $\rho_k$.
This prevents $\Delta_k$ from decreasing too quickly until the algorithm is certain that $Y$ is sufficiently well-conditioned.\autocite{Powell2003MP}
Another critical feature of Py-BOBYQA is the implementation of multiple restarts, which endows it with greater robustness towards noise and also allows it to escape local minima.\autocite{Cartis2019ACMTMS,Cartis2022O}
However, the multiple-restarts feature in Py-BOBYQA was disabled in POISE as this often led to overly long optimisations.%
\footnote{Most mathematics papers on optimisation have no qualms in using hundreds or even thousands of FEs, and it is this context in which Py-BOBYQA outperforms other algorithms. Unfortunately for me, POISE works in an \textit{extremely} restrictive regime where even 50 FEs would be considered very expensive.}

Crucially, Py-BOBYQA differs from the simplex-based methods in that \textit{it cares about the actual value of the cost function}.
In the NM and MDS methods, only the relative ordering of the points in the simplex matters; it makes no difference to the algorithm whether the worst point has a cost function value of 10 or 1000.
However, in Py-BOBYQA, the value of $f$ is used in constructing the model, and thus directly influences the optimisation trajectory.
Although this is beneficial in cases where the underlying cost function is relatively well-behaved (this \textit{probably} means cases where the cost function is well described by a quadratic model\footnote{Of course, because of Taylor's theorem, every non-noisy cost function can be locally described by a quadratic model within a sufficiently small region. However, for meaningful progress to be made with noisy cost functions, the model must be built over a large enough region such that noise becomes less relevant.}), and is reflected in faster convergence rates, it can be problematic for some cost functions.
Py-BOBYQA is set as the default optimiser in POISE, but the user is strongly recommended to try the NM method as a first step when troubleshooting failed optimisations.

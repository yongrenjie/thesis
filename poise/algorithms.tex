\subsection{Optimisation algorithms}
\label{subsec:poise__algorithms}

As was briefly mentioned in \cref{subsec:pureshift__optim_techniques}, derivative-based optimisation algorithms cannot be used for experimental optimisations.
To be more precise, when analytical gradients are not available, derivative-based algorithms calculate gradients using a finite difference approximation:
\begin{equation}
    \label{eq:finite_difference_grad}
    \nabla f(x) \approx \frac{f(x + \varepsilon) - f(x - \varepsilon)}{2\varepsilon},
\end{equation}
where $\varepsilon$ is the step size used for the finite difference calculation.
In Nocedal and Wright\autocite{Nocedal2006}, it is shown that the error in this finite difference gradient (as compared to the true gradient) has an upper bound of
\begin{equation}
    \label{eq:finite_difference_error}
    \frac{\eta(x;\varepsilon)}{\varepsilon} + O(\varepsilon^2),
\end{equation}
where $\eta(x;\varepsilon)$ is (roughly) the noise in the region $[x - \varepsilon, x + \varepsilon]$.
The first term represents the error due to this noise, and if $\varepsilon$ is small, this term is large;
on the other hand, the second term represents the error due to the finite difference approximation, and if $\varepsilon$ is large \textit{this} term is large.
This means that finite difference gradients, and any algorithms which use them, are entirely unreliable in the presence of noise.
Instead, POISE provides a choice of three derivative-free optimisation algorithms: the \acf{nm} method\autocite{Nelder1965TCJ}, the \acf{mds} method\autocite{Torczon1989,DennisJr1991SIAMJO}, and the Py-BOBYQA trust-region method\autocite{Powell2009Proc,Cartis2019ACMTMS}.


\input{poise/nelder_mead.tex}
\input{poise/multidirectional_search.tex}
\input{poise/pybobyqa.tex}
